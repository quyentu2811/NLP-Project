{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bbfabb7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-21T05:21:29.168097Z",
     "iopub.status.busy": "2024-04-21T05:21:29.167753Z",
     "iopub.status.idle": "2024-04-21T05:21:29.813490Z",
     "shell.execute_reply": "2024-04-21T05:21:29.812699Z"
    },
    "papermill": {
     "duration": 0.65254,
     "end_time": "2024-04-21T05:21:29.815817",
     "exception": false,
     "start_time": "2024-04-21T05:21:29.163277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaf68c3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T05:21:29.822642Z",
     "iopub.status.busy": "2024-04-21T05:21:29.822016Z",
     "iopub.status.idle": "2024-04-21T05:21:32.705596Z",
     "shell.execute_reply": "2024-04-21T05:21:32.704367Z"
    },
    "papermill": {
     "duration": 2.88933,
     "end_time": "2024-04-21T05:21:32.707952",
     "exception": false,
     "start_time": "2024-04-21T05:21:29.818622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'dialogue-text-summarization': No such file or directory\r\n",
      "rm: cannot remove 'flant5-base-final': No such file or directory\r\n",
      "rm: cannot remove 'wandb': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm -r dialogue-text-summarization\n",
    "!rm -r flant5-base-final\n",
    "!rm -r wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8252079f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T05:21:32.715283Z",
     "iopub.status.busy": "2024-04-21T05:21:32.714979Z",
     "iopub.status.idle": "2024-04-21T05:21:34.529423Z",
     "shell.execute_reply": "2024-04-21T05:21:34.528461Z"
    },
    "papermill": {
     "duration": 1.820796,
     "end_time": "2024-04-21T05:21:34.531779",
     "exception": false,
     "start_time": "2024-04-21T05:21:32.710983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'dialogue-text-summarization'...\r\n",
      "remote: Enumerating objects: 354, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (51/51), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (43/43), done.\u001b[K\r\n",
      "remote: Total 354 (delta 10), reused 17 (delta 6), pack-reused 303\u001b[K\r\n",
      "Receiving objects: 100% (354/354), 672.12 KiB | 21.00 MiB/s, done.\r\n",
      "Resolving deltas: 100% (176/176), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone -b dev/truong https://github.com/dtruong46me/dialogue-text-summarization.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07813a2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T05:21:34.540052Z",
     "iopub.status.busy": "2024-04-21T05:21:34.539737Z",
     "iopub.status.idle": "2024-04-21T05:22:27.026529Z",
     "shell.execute_reply": "2024-04-21T05:22:27.025325Z"
    },
    "papermill": {
     "duration": 52.493745,
     "end_time": "2024-04-21T05:22:27.028897",
     "exception": false,
     "start_time": "2024-04-21T05:21:34.535152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xin chao anh em!\r\n",
      "Chung toi dang setup moi truong ...\r\n",
      "...\r\n",
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: '/content/dialogue-text-summarization/requirements.txt'\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\r\n",
      "\u001b[0m---------\r\n",
      "Set up complete!\r\n"
     ]
    }
   ],
   "source": [
    "!bash \"/kaggle/working/dialogue-text-summarization/setup.sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a58ae700",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T05:22:27.037978Z",
     "iopub.status.busy": "2024-04-21T05:22:27.037624Z",
     "iopub.status.idle": "2024-04-21T05:50:56.779578Z",
     "shell.execute_reply": "2024-04-21T05:50:56.778470Z"
    },
    "papermill": {
     "duration": 1709.749506,
     "end_time": "2024-04-21T05:50:56.782191",
     "exception": false,
     "start_time": "2024-04-21T05:22:27.032685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-21 05:22:37.073527: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-04-21 05:22:37.073625: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-04-21 05:22:37.173874: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\r\n",
      "Token is valid (permission: write).\r\n",
      "Your token has been saved to /root/.cache/huggingface/token\r\n",
      "Login successful\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mquyentu2811\u001b[0m (\u001b[33mhoangtuquyen\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\r\n",
      "tokenizer_config.json: 100%|███████████████| 2.54k/2.54k [00:00<00:00, 12.9MB/s]\r\n",
      "spiece.model: 100%|██████████████████████████| 792k/792k [00:00<00:00, 1.51MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 2.42M/2.42M [00:00<00:00, 6.04MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████| 2.20k/2.20k [00:00<00:00, 10.6MB/s]\r\n",
      "config.json: 100%|█████████████████████████| 1.40k/1.40k [00:00<00:00, 7.44MB/s]\r\n",
      "model.safetensors: 100%|█████████████████████| 990M/990M [00:24<00:00, 40.6MB/s]\r\n",
      "generation_config.json: 100%|███████████████████| 147/147 [00:00<00:00, 735kB/s]\r\n",
      "Downloading readme: 100%|██████████████████| 4.65k/4.65k [00:00<00:00, 15.3MB/s]\r\n",
      "Downloading data: 100%|████████████████████| 11.3M/11.3M [00:00<00:00, 19.4MB/s]\r\n",
      "Downloading data: 100%|██████████████████████| 442k/442k [00:00<00:00, 1.07MB/s]\r\n",
      "Downloading data: 100%|████████████████████| 1.35M/1.35M [00:00<00:00, 5.51MB/s]\r\n",
      "Generating train split: 100%|███| 12460/12460 [00:00<00:00, 64844.73 examples/s]\r\n",
      "Generating validation split: 100%|██| 500/500 [00:00<00:00, 41302.85 examples/s]\r\n",
      "Generating test split: 100%|██████| 1500/1500 [00:00<00:00, 83067.59 examples/s]\r\n",
      "Map: 100%|█████████████████████████| 12460/12460 [02:55<00:00, 70.80 examples/s]\r\n",
      "Map: 100%|█████████████████████████████| 500/500 [00:06<00:00, 71.46 examples/s]\r\n",
      "Map: 100%|███████████████████████████| 1500/1500 [00:21<00:00, 71.18 examples/s]\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.5\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240421_052650-dqscb089\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mflan-t5-base-final-run\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/hoangtuquyen/huggingface\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/hoangtuquyen/huggingface/runs/dqscb089/workspace\u001b[0m\r\n",
      "{'loss': 0.125, 'grad_norm': 0.08127518743276596, 'learning_rate': 4.464438731790917e-05, 'epoch': 0.64}\r\n",
      " 11%|████                                  | 250/2334 [23:29<3:15:25,  5.63s/it]\r\n",
      "  0%|                                                   | 0/250 [00:00<?, ?it/s]\u001b[A\r\n",
      "  1%|▎                                          | 2/250 [00:00<00:14, 16.72it/s]\u001b[A\r\n",
      "  2%|▋                                          | 4/250 [00:00<00:23, 10.42it/s]\u001b[A\r\n",
      "  2%|█                                          | 6/250 [00:00<00:26,  9.11it/s]\u001b[A\r\n",
      "  3%|█▏                                         | 7/250 [00:00<00:27,  8.84it/s]\u001b[A\r\n",
      "  3%|█▍                                         | 8/250 [00:00<00:28,  8.61it/s]\u001b[A\r\n",
      "  4%|█▌                                         | 9/250 [00:00<00:28,  8.44it/s]\u001b[A\r\n",
      "  4%|█▋                                        | 10/250 [00:01<00:52,  4.61it/s]\u001b[A\r\n",
      "  4%|█▊                                        | 11/250 [00:01<00:46,  5.17it/s]\u001b[A\r\n",
      "  5%|██                                        | 12/250 [00:01<00:41,  5.73it/s]\u001b[A\r\n",
      "  5%|██▏                                       | 13/250 [00:01<00:38,  6.19it/s]\u001b[A\r\n",
      "  6%|██▎                                       | 14/250 [00:01<00:35,  6.56it/s]\u001b[A\r\n",
      "  6%|██▌                                       | 15/250 [00:02<00:34,  6.84it/s]\u001b[A\r\n",
      "  6%|██▋                                       | 16/250 [00:02<00:40,  5.83it/s]\u001b[A\r\n",
      "  7%|██▊                                       | 17/250 [00:02<00:43,  5.40it/s]\u001b[A\r\n",
      "  7%|███                                       | 18/250 [00:02<00:39,  5.89it/s]\u001b[A\r\n",
      "  8%|███▏                                      | 19/250 [00:02<00:36,  6.28it/s]\u001b[A\r\n",
      "  8%|███▎                                      | 20/250 [00:03<00:51,  4.48it/s]\u001b[A\r\n",
      "  8%|███▌                                      | 21/250 [00:03<00:46,  4.94it/s]\u001b[A\r\n",
      "  9%|███▋                                      | 22/250 [00:03<00:41,  5.46it/s]\u001b[A\r\n",
      "  9%|███▊                                      | 23/250 [00:03<00:56,  4.02it/s]\u001b[A\r\n",
      " 10%|████                                      | 24/250 [00:04<00:48,  4.63it/s]\u001b[A\r\n",
      " 10%|████▏                                     | 25/250 [00:04<00:57,  3.93it/s]\u001b[A\r\n",
      " 10%|████▎                                     | 26/250 [00:04<00:49,  4.54it/s]\u001b[A\r\n",
      " 11%|████▌                                     | 27/250 [00:04<00:58,  3.82it/s]\u001b[A\r\n",
      " 11%|████▋                                     | 28/250 [00:05<00:50,  4.43it/s]\u001b[A\r\n",
      " 12%|████▊                                     | 29/250 [00:05<01:00,  3.68it/s]\u001b[A\r\n",
      " 12%|█████                                     | 30/250 [00:05<00:51,  4.28it/s]\u001b[A\r\n",
      " 12%|█████▏                                    | 31/250 [00:05<01:01,  3.53it/s]\u001b[A\r\n",
      " 13%|█████▍                                    | 32/250 [00:06<01:01,  3.55it/s]\u001b[A\r\n",
      " 13%|█████▌                                    | 33/250 [00:06<01:01,  3.53it/s]\u001b[A\r\n",
      " 14%|█████▋                                    | 34/250 [00:06<01:01,  3.51it/s]\u001b[A\r\n",
      " 14%|█████▉                                    | 35/250 [00:07<01:02,  3.46it/s]\u001b[A\r\n",
      " 14%|██████                                    | 36/250 [00:07<01:02,  3.41it/s]\u001b[A\r\n",
      " 15%|██████▏                                   | 37/250 [00:07<01:03,  3.36it/s]\u001b[A\r\n",
      " 15%|██████▍                                   | 38/250 [00:08<01:04,  3.31it/s]\u001b[A\r\n",
      " 16%|██████▌                                   | 39/250 [00:08<01:04,  3.26it/s]\u001b[A\r\n",
      " 16%|██████▋                                   | 40/250 [00:08<01:05,  3.20it/s]\u001b[A\r\n",
      " 16%|██████▉                                   | 41/250 [00:08<01:06,  3.15it/s]\u001b[A\r\n",
      " 17%|███████                                   | 42/250 [00:09<01:07,  3.09it/s]\u001b[A\r\n",
      " 17%|███████▏                                  | 43/250 [00:09<01:08,  3.04it/s]\u001b[A\r\n",
      " 18%|███████▍                                  | 44/250 [00:10<01:08,  2.99it/s]\u001b[A\r\n",
      " 18%|███████▌                                  | 45/250 [00:10<01:09,  2.95it/s]\u001b[A\r\n",
      " 18%|███████▋                                  | 46/250 [00:10<01:10,  2.90it/s]\u001b[ATraceback (most recent call last):\r\n",
      "  File \"/kaggle/working/dialogue-text-summarization/run_training.py\", line 48, in <module>\r\n",
      "    training_pipeline(args)\r\n",
      "  File \"/kaggle/working/dialogue-text-summarization/src/pipelines/training_pipeline.py\", line 55, in training_pipeline\r\n",
      "    raise e\r\n",
      "  File \"/kaggle/working/dialogue-text-summarization/src/pipelines/training_pipeline.py\", line 46, in training_pipeline\r\n",
      "    trainer.train()\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1780, in train\r\n",
      "    return inner_training_loop(\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2193, in _inner_training_loop\r\n",
      "    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2577, in _maybe_log_save_evaluate\r\n",
      "    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer_seq2seq.py\", line 180, in evaluate\r\n",
      "    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 3365, in evaluate\r\n",
      "    output = eval_loop(\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 3580, in evaluation_loop\r\n",
      "    preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py\", line 138, in nested_concat\r\n",
      "    return type(tensors)(nested_concat(t, n, padding_index=padding_index) for t, n in zip(tensors, new_tensors))\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py\", line 138, in <genexpr>\r\n",
      "    return type(tensors)(nested_concat(t, n, padding_index=padding_index) for t, n in zip(tensors, new_tensors))\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py\", line 140, in nested_concat\r\n",
      "    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py\", line 99, in torch_pad_and_concatenate\r\n",
      "    return torch.cat((tensor1, tensor2), dim=0)\r\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.76 GiB. GPU 0 has a total capacty of 15.89 GiB of which 5.55 GiB is free. Process 2775 has 10.35 GiB memory in use. Of the allocated memory 8.69 GiB is allocated by PyTorch, and 1.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/kaggle/working/dialogue-text-summarization/run_training.py\", line 48, in <module>\r\n",
      "    training_pipeline(args)\r\n",
      "  File \"/kaggle/working/dialogue-text-summarization/src/pipelines/training_pipeline.py\", line 55, in training_pipeline\r\n",
      "    raise e\r\n",
      "  File \"/kaggle/working/dialogue-text-summarization/src/pipelines/training_pipeline.py\", line 46, in training_pipeline\r\n",
      "    trainer.train()\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1780, in train\r\n",
      "    return inner_training_loop(\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2193, in _inner_training_loop\r\n",
      "    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2577, in _maybe_log_save_evaluate\r\n",
      "    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer_seq2seq.py\", line 180, in evaluate\r\n",
      "    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 3365, in evaluate\r\n",
      "    output = eval_loop(\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 3580, in evaluation_loop\r\n",
      "    preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py\", line 138, in nested_concat\r\n",
      "    return type(tensors)(nested_concat(t, n, padding_index=padding_index) for t, n in zip(tensors, new_tensors))\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py\", line 138, in <genexpr>\r\n",
      "    return type(tensors)(nested_concat(t, n, padding_index=padding_index) for t, n in zip(tensors, new_tensors))\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py\", line 140, in nested_concat\r\n",
      "    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py\", line 99, in torch_pad_and_concatenate\r\n",
      "    return torch.cat((tensor1, tensor2), dim=0)\r\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.76 GiB. GPU 0 has a total capacty of 15.89 GiB of which 5.55 GiB is free. Process 2775 has 10.35 GiB memory in use. Of the allocated memory 8.69 GiB is allocated by PyTorch, and 1.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/epoch ▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/global_step ▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train/grad_norm ▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/learning_rate ▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss ▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/epoch 0.64\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/global_step 250\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train/grad_norm 0.08128\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/learning_rate 4e-05\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 0.125\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mflan-t5-base-final-run\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/hoangtuquyen/huggingface/runs/dqscb089/workspace\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240421_052650-dqscb089/logs\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/dialogue-text-summarization/run_training.py\\\n",
    "                --checkpoint \"google/flan-t5-base\"\\\n",
    "                --datapath \"knkarthick/dialogsum\"\\\n",
    "                --huggingface_hub_token \"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\"\\\n",
    "                --wandb_token \"c456a83d581dce980c77ae1bf7613173858779c8\"\\\n",
    "                --output_dir \"flant5-base-final\"\\\n",
    "                --overwrite_output_dir True\\\n",
    "                --run_name \"flan-t5-base-final-run\"\\\n",
    "                --num_train_epochs 6\\\n",
    "                --per_device_train_batch_size 2\\\n",
    "                --per_device_eval_batch_size 2\\\n",
    "                --logging_steps 250\\\n",
    "                --gradient_accumulation_steps 16\\\n",
    "                --save_total_limit 1\\\n",
    "                --evaluation_strategy \"steps\"\\\n",
    "                --metric_for_best_model \"eval_loss\"\\\n",
    "                --save_strategy \"steps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836b9f64",
   "metadata": {
    "papermill": {
     "duration": 0.041872,
     "end_time": "2024-04-21T05:50:56.863994",
     "exception": false,
     "start_time": "2024-04-21T05:50:56.822122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1770.858004,
   "end_time": "2024-04-21T05:50:57.327419",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-21T05:21:26.469415",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
